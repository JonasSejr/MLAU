{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handin 1\n",
    "## Simple Logistic Regression\n",
    "### Implementation\n",
    "#### Batch Gradient Descent\n",
    "#### Mini Batch Gradient Descent\n",
    "### Termination\n",
    "### Possible optimizations\n",
    "Some calculations are done twize...\n",
    "### Numerical issues\n",
    "The fix...\n",
    "\n",
    "###Theoretical questions\n",
    "Sanity Check: What happens if we randomly permute the pixels in each image (with the same permutation) before we train the classifier? Will we get a classifier that is better, worse, or the same? Give a short explanation.\n",
    "\n",
    "Linear Separable: If the data is linearly separable, what happens to weights when we implement logistic regression with gradient descent? That is, how do the weights that minimize the negative log likelihood look like? Assume that we have full precision (that is, ignore floating point errors). We can run gradient descent on the data set for as long as we want (suppose God helps you). Now what will happen with the weights in the limit? Do they converge to some fixed number (fluctuate around it) or do they keep increasing in magnitude (absolute value)? Give a short explanation for your answer. What happens if we add regularization?\n",
    "\n",
    "Bonus Question: Convexity of negative log likelihood Show that the negative log likelihood function for logistic regression is convex. Is it still convex if we add regularization?\n",
    "\n",
    "## Softmax logistic regression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
